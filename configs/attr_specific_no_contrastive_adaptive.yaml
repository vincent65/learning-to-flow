# ADAPTIVE CONFIG - Fewer steps to prevent overshooting
#
# Hypothesis: Model overshoots target in first few steps, then oscillates back
# Solution: Use fewer steps (5 instead of 10) with smaller step size
#
# Changes from balanced:
# - flow_steps: 10 → 5 (fewer steps)
# - step_size: 0.1 → 0.08 (smaller steps)
# - Total distance: 1.0 → 0.4 (much less travel)

model:
  embedding_dim: 512
  num_attributes: 5
  hidden_dim: 256
  projection_radius: 1.0

data:
  celeba_root: "data/celeba"
  embedding_dir: "data/embeddings"

training:
  num_epochs: 100
  batch_size: 512
  learning_rate: 0.0001
  alpha: 0.08  # Smaller training steps
  flow_steps: 5  # Fewer steps during training
  use_no_contrastive: true

loss:
  lambda_classifier: 2.0
  lambda_orthogonal: 0.1
  lambda_identity: 0.2
  lambda_smoothness: 0.1
  lambda_curl: 0.0
  lambda_div: 0.0

inference:
  num_flow_steps: 5  # Fewer steps during inference
  step_size: 0.08
