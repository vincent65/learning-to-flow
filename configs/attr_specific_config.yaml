# Configuration for Attribute-Specific Vector Field Training
#
# Key differences from original FCLF:
# 1. Separate vector field per attribute (avoids 2^N discrete clustering)
# 2. Per-attribute contrastive loss (not all-or-nothing)
# 3. Orthogonality loss (attribute independence)
# 4. Smoother clustering with softer temperature

model:
  embedding_dim: 512
  num_attributes: 5
  hidden_dim: 256
  projection_radius: 1.0

data:
  celeba_root: "data/celeba"
  embedding_dir: "data/embeddings"

training:
  num_epochs: 100
  batch_size: 512  # Same as original
  learning_rate: 0.0001
  alpha: 0.12  # Step size for flow (slightly smaller than original 0.15)
  flow_steps: 10  # Number of steps during training

loss:
  # Per-attribute contrastive (softer than original)
  temperature: 0.2  # Original: 0.1, we use softer
  lambda_contrastive: 0.5  # Weight for clustering

  # Attribute independence
  lambda_orthogonal: 0.1  # Encourages attribute vector fields to be orthogonal

  # Manifold preservation
  lambda_identity: 0.2  # Stay near original embeddings

  # Trajectory smoothness
  lambda_smoothness: 0.1  # Penalize jerky trajectories

  # Optional regularization (can enable if needed)
  lambda_curl: 0.0  # Set to 0.01 if you want irrotational flows
  lambda_div: 0.0   # Set to 0.01 if you want incompressible flows

inference:
  num_flow_steps: 10
  step_size: 0.1
