# NO CONTRASTIVE LOSS - Prevents mode collapse by avoiding clustering
#
# Key innovation: Replace contrastive loss with classifier loss
# - Contrastive loss pulls embeddings into discrete clusters → mode collapse
# - Classifier loss just verifies attributes are correct → no clustering!
#
# This should maintain high uniqueness even after 100 epochs.

model:
  embedding_dim: 512
  num_attributes: 5
  hidden_dim: 256
  projection_radius: 1.0

data:
  celeba_root: "data/celeba"
  embedding_dir: "data/embeddings"

training:
  num_epochs: 100
  batch_size: 512
  learning_rate: 0.0001
  alpha: 0.15  # Slightly larger steps since no contrastive pull
  flow_steps: 10
  use_no_contrastive: true  # CRITICAL: Use new loss function

loss:
  # NEW: Classifier loss instead of contrastive
  lambda_classifier: 1.0  # Ensure correct attribute classification

  # Attribute independence
  lambda_orthogonal: 0.1

  # Manifold preservation (stronger since no contrastive anchor)
  lambda_identity: 0.5  # Increased from 0.2

  # Trajectory smoothness
  lambda_smoothness: 0.1

  # Optional regularization
  lambda_curl: 0.0
  lambda_div: 0.0

inference:
  num_flow_steps: 10
  step_size: 0.1
